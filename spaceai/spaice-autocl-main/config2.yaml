# Runtime params
#===================================
stage: "1HPO" #
hpo_type: "adaptive"
model_architecture: "ESN" # "LSTM" # neural network architecture LSTM, ESN
num_sampler : 50  #
num_sampler2 : 10  #
cpu: 2 #
cpu_adaptive:  8 #

train: True # train new or existing model for each channel
predict: True # generate new predicts or, if False, use predictions stored locally
use_id:
cuda_id: 0

# number of values to evaluate in each batch
batch_size: 70

# number of trailing batches to use in error calculation
window_size: 30

# Columns headers for output file
header: ["run_id", "chan_id", "spacecraft", "num_anoms", "anomaly_sequences", "class", "true_positives",
        "false_positives", "false_negatives", "tp_sequences", "fp_sequences", "gaussian_p-value", "num_values",
        "normalized_error", "eval_time", "scores"]

# determines window size used in EWMA smoothing (percentage of total values for channel)
smoothing_perc: 0.05

# number of values surrounding an error that are brought into the sequence (promotes grouping on nearby sequences
error_buffer: 100

# LSTM/ESN parameters
# model parameters
# ==================================
loss_metric: 'mse'
optimizer: 'adam'
learning_rate: '0.001'
validation_split: 0.2
dropout: 0.3
lstm_batch_size: 64
esn_batch_number: 32
weight_decay: 0
# maximum number of epochs allowed (if early stopping criteria not met)
epochs: 15

# network architecture [<neurons in hidden layer>, <neurons in hidden layer>]
# Size of input layer not listed - dependent on evr modules and types included (see 'evr_modules' and 'erv_types' above)
layers: [80,80]

# Number of consequetive training iterations to allow without decreasing the val_loss by at least min_delta
patience: 5
min_delta: 0.0003

# num previous timesteps provided to model to predict future values
l_s: 250

# number of steps ahead to predict
n_predictions: 10


# === Parameters only for ESN ===
# Name of the activation function from `torch` (e.g. `torch.tanh`)
activation: tanh

# The value of the leaking parameter `alpha`
leakage: 1

# The value for the desired scaling of the input (must be `<= 1`)
input_scaling: 0.9

# The desired spectral radius of the recurrent matrix (must be `< 1`)
rho: 0.99

# The kind of initialization of the input transformation. Default: `'uniform'`
kernel_initializer: uniform

# The kind of initialization of the recurrent matrix. Default: `'normal'`
recurrent_initializer: normal

# If ``True``, the network uses additional ``g`` (gain) and ``b`` (bias) parameters. Default: ``False``
net_gain_and_bias: False

# If ``False``, the layer does not use bias weights `b`
bias: False

# The value of l2 regularization
l2: 1e-10

# Error thresholding parameters
# ==================================

# minimum percent decrease between max errors in anomalous sequences (used for pruning)
p: 0.13
